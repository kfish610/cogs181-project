{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "import requests\n",
    "from ast import literal_eval\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "import torch\n",
    "from torch.optim.sgd import SGD\n",
    "from torch.nn import Module, LSTM, Linear, Softmax, CrossEntropyLoss\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by loading card data from [Scryfall](https://scryfall.com/), a Magic: The Gathering search engine and data aggregator. We clean the data by removing weirdly formatted cards, as well as cards that contain rarely used characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards_raw = pd.read_json('data/scryfall-data.zip')\n",
    "\n",
    "# Remove all multi-faced or other weirdly formatted cards\n",
    "cards_raw = cards_raw[cards_raw['layout'] == 'normal']\n",
    "\n",
    "# Remove all digital-only cards\n",
    "cards_raw = cards_raw[~cards_raw['digital']]\n",
    "\n",
    "# Remove all joke cards\n",
    "cards_raw = cards_raw[cards_raw['set_type'] != 'funny']\n",
    "\n",
    "# Remove cards with no text\n",
    "cards_raw = cards_raw[cards_raw['oracle_text'].str.len() > 0]\n",
    "\n",
    "### The next few steps reduce the number of characters we will have to one-hot encode ###\n",
    "# Remove cards that have uncommon characters\n",
    "cards_raw = cards_raw[~cards_raw['oracle_text'].str.contains(r'[!%?úíÉ\\[\\]]')]\n",
    "\n",
    "# Fix index\n",
    "cards_raw = cards_raw.reset_index(drop=True)\n",
    "\n",
    "# Replace \"minus\" with \"hyphen\", as they fulfil the same purpose\n",
    "cards_raw['oracle_text'] = cards_raw['oracle_text'].str.replace('−', '-')\n",
    "\n",
    "# Replace semicolon with comma, as they are close enough\n",
    "cards_raw['oracle_text'] = cards_raw['oracle_text'].str.replace(';', ',')\n",
    "\n",
    "### Generalize card names appearing in rules text ###\n",
    "# This is because the name of the card is irrelevant to its effect, \n",
    "# and cards can even be reprinted with different names\n",
    "def generalize_name(card):\n",
    "    generalized = card.copy()\n",
    "    generalized['oracle_text'] = card['oracle_text'].replace(card['name'], '~')\n",
    "    return generalized\n",
    "\n",
    "cards = cards_raw.apply(generalize_name, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then load data from EDHREC, another data aggregator, which has tags on many cards relating to their purpose (e.g. removing threats, playing more mana, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load EDHREC data; if it doesn't exist, regenerate it\n",
    "try:\n",
    "    edhrec_tags = pd.read_csv('data/edhrec_data.csv', index_col=0).applymap(literal_eval, na_action='ignore')\n",
    "except:\n",
    "    # Replace all special characters with hyphens, other than apostrophes, to match EDHREC format\n",
    "    edhrec_names = cards['name'].str.lower().str.replace('\\'', '').str.replace(r'\\W+', '-', regex=True)\n",
    "\n",
    "    # Split the names into 300 card chunks to fit EDHREC API requirements, then query the API\n",
    "    edhrec_name_chunks = np.array_split(edhrec_names.to_numpy(), range(300, edhrec_names.shape[0], 300))\n",
    "    edhrec_results = [ requests.post('https://edhrec.com/api/cards/', json={'format': 'dict', 'names': list(chunk)}).json()['cards'] for chunk in edhrec_name_chunks ]\n",
    "\n",
    "    # Get the tags for the cards\n",
    "    edhrec_tags = pd.concat([pd.DataFrame(res).T for res in edhrec_results ])['tags']\n",
    "    # Reindex tags based on card name\n",
    "    edhrec_tags = pd.merge(edhrec_names, edhrec_tags, left_on='name', right_index=True)['tags']\n",
    "\n",
    "    # Save to CSV\n",
    "    edhrec_tags.to_csv('data/edhrec_data.csv')\n",
    "\n",
    "# Add the tags to our card data\n",
    "cards_tagged = cards.join(edhrec_tags)\n",
    "# Drop cards with no tags\n",
    "cards_tagged = cards_tagged[~cards_tagged['tags'].isnull()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we load the \"Oracle text\" of the cards (the text of the cards under the rules, including errata and updates for uniformity). We convert it into one-hot encoding by characters, then into a packed sequence of tensors for batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The size of each character tensor\n",
    "INPUT_SIZE = 80\n",
    "\n",
    "# Get the frequency of each character in the corpus\n",
    "char_freqs = pd.Series(list(''.join(cards_tagged['oracle_text']))).value_counts()\n",
    "# Ensure we only have 80 characters appearing\n",
    "assert(char_freqs.shape[0] == INPUT_SIZE)\n",
    "\n",
    "# Mapping from a character into its index\n",
    "char_indices = pd.Series(range(INPUT_SIZE), index=char_freqs.index)\n",
    "\n",
    "# Turn the text of all cards into a packed (jagged) tensor\n",
    "tensor_text = [ torch.tensor(np.identity(INPUT_SIZE)[char_indices[list(text)]]).float() for text in cards_tagged['oracle_text'] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we load the tags into a tensor to be our labels/training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_SIZE = 7\n",
    "\n",
    "# Get the frequencies of each category\n",
    "tag_freqs = cards_tagged['tags'].explode().value_counts()\n",
    "# Ensure we only have 7 categories\n",
    "assert(tag_freqs.shape[0] == OUTPUT_SIZE)\n",
    "\n",
    "# Mapping from a tag into its index\n",
    "tag_indices = pd.Series(range(OUTPUT_SIZE), index=tag_freqs.index)\n",
    "\n",
    "# One-hot encoding, but with the possibility of multiple categories\n",
    "tensor_tags = [ torch.tensor(np.identity(OUTPUT_SIZE)[tag_indices[tags]].mean(axis=0)).float() for tags in cards_tagged['tags'] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create the model, consisting of an LSTM layer, followed by a linear layer and a sigmoid activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTGClassifier(Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MTGClassifier, self).__init__()\n",
    "        self.input_size, self.hidden_size, self.output_size = input_size, hidden_size, output_size\n",
    "\n",
    "        self.lstm = LSTM(input_size, hidden_size)\n",
    "        self.linear = Linear(hidden_size, output_size)\n",
    "        self.activation = Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        out, _ = self.lstm(input)\n",
    "        out = self.linear(out)\n",
    "        out = self.activation(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN version incompatibility: PyTorch was compiled  against (8, 7, 0) but found runtime version (8, 6, 0). PyTorch already comes bundled with cuDNN. One option to resolving this error is to ensure PyTorch can find the bundled cuDNN.Looks like your LD_LIBRARY_PATH contains incompatible version of cudnnPlease either remove it from the path or install cudnn (8, 7, 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_118/1110042081.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMTGClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINPUT_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHIDDEN_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Inversely weight the categories, so that we don't get too much focus on the largest one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_118/2053195968.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_size, hidden_size, output_size)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LSTM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_expected_cell_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mode, input_size, hidden_size, num_layers, bias, batch_first, dropout, bidirectional, proj_size, device, dtype)\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_all_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_flat_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36m_init_flat_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    153\u001b[0m         self._flat_weight_refs = [weakref.ref(w) if w is not None else None\n\u001b[1;32m    154\u001b[0m                                   for w in self._flat_weights]\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mflatten_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m             if (not isinstance(fw.data, Tensor) or not (fw.data.dtype == dtype) or\n\u001b[1;32m    184\u001b[0m                     \u001b[0;32mnot\u001b[0m \u001b[0mfw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_cuda\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                     not torch.backends.cudnn.is_acceptable(fw.data)):\n\u001b[0m\u001b[1;32m    186\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/backends/cudnn/__init__.py\u001b[0m in \u001b[0;36mis_acceptable\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m    108\u001b[0m         )\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         warnings.warn(\n\u001b[1;32m    112\u001b[0m             \"cuDNN/MIOpen library not found. Check your {libpath}\".format(\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/backends/cudnn/__init__.py\u001b[0m in \u001b[0;36m_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m                         \u001b[0msubstring\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mld_library_path\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msubstring\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cudnn\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                     ):\n\u001b[0;32m---> 58\u001b[0;31m                         raise RuntimeError(\n\u001b[0m\u001b[1;32m     59\u001b[0m                             \u001b[0;34mf\"{base_error_msg}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                             \u001b[0;34mf\"Looks like your LD_LIBRARY_PATH contains incompatible version of cudnn\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuDNN version incompatibility: PyTorch was compiled  against (8, 7, 0) but found runtime version (8, 6, 0). PyTorch already comes bundled with cuDNN. One option to resolving this error is to ensure PyTorch can find the bundled cuDNN.Looks like your LD_LIBRARY_PATH contains incompatible version of cudnnPlease either remove it from the path or install cudnn (8, 7, 0)"
     ]
    }
   ],
   "source": [
    "HIDDEN_SIZE = 20\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "model = MTGClassifier(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "\n",
    "# Inversely weight the categories, so that we don't get too much focus on the largest one\n",
    "inverse_freqs = 1/tag_freqs\n",
    "weights = inverse_freqs/inverse_freqs.sum()\n",
    "loss_func = CrossEntropyLoss(weight=torch.tensor(weights.to_numpy()))\n",
    "\n",
    "optimizer = SGD(model.parameters(), lr=0.05)\n",
    "\n",
    "# Split data into 80/20 train/test\n",
    "text_tagged = list(zip(tensor_text, tensor_tags))\n",
    "train_data, test_data = random_split(text_tagged, [ 0.8, 0.2 ])\n",
    "\n",
    "for text, tags in train_data:\n",
    "    # Clear accumulated gradient\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Calculate probabilities from sequence\n",
    "    probabilities = model(text)\n",
    "\n",
    "    loss = loss_func(probabilities[-1], tags)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(test_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[0][1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
